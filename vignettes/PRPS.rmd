---
title: "Package PRPS"
output: rmarkdown::html_vignette
vignette: >

  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Package PRPS}
  %\VignetteEncoding{UTF-8}
  \usepackage[utf8]{inputenc}
  \VignetteEngine{knitr::knitr}

---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
options(tibble.print_min = 4L, tibble.print_max = 4L)
library(dplyr)
library(ggplot2)
set.seed(1014)
```

Type: Package

Title: Calculate classification scores and classify samples into 2 to 3 groups

Version: 0.1.0

Author: Aixiang Jiang, ..., David Scott, Ryan Morin

Maintainer: Aixiang Jiang <aijiang@bccrc.ca>

Depends: R (>= 3.3.1), lattice, caret, limma, e1071

Suggests: knitr

VignetteBuilder: knir

&nbsp;
&nbsp;

# I. Introduction

This package calculates classification prediction score with three method choices: 

##   1. LPS (Linear Prediction Score); 
   In the classification step, if LPS is chosen, Empirical Bayes' probabilities are calcualted 
   and classification is based on cutoff on probabilities;

#### References:

Wright G, Tan B, Rosenwald A, Hurt EH, Wiestner A, Staudt LM. A trait expression-based method
to diagnose clinically distinct subgroups of diffuse large B cell lymphoma. Proc Natl Acad Sci U S
A. 2003 Aug 19;100(17):9991-6.

   
##  2. PRPS (Probability ratio based classification predication score);
 if PRPS is chosen, two types of outputs
   are given: one is based on cutoff on Empirical Bayes' probabilities, the other one is based on 
   natural cutoff 0 on PRPS scores;
  
####  References:

Ennishi D, Jiang A, Boyle M, Collinge B, Grande BM, Ben-Neriah S, Rushton C, Tang J, Thomas N, Slack GW, Farinha P, Takata K, Miyata-Takata T, Craig J, Mottok A, Meissner B, Saberi S, Bashashati A, Villa D, Savage KJ, Sehn LH, Kridel R, Mungall AJ, Marra MA, Shah SP, Steidl C, Connors JM, Gascoyne RD, Morin RD, Scott DW. Double-Hit Trait Expression Signature Defines a Distinct Subgroup of Germinal Center B-Cell-Like Diffuse Large B-Cell
Lymphoma. J Clin Oncol. 2018 Dec 3:JCO1801583. doi: 10.1200/JCO.18.01583.
   
##  3. PS (Prediction Strength).
   
 when PS is selected, by default, classification is based on 
   natural cutoff 0 on PS scores, however, a separate function can alos issues classification based
   on cutoff on Empirical Bayes' probabilities when necessary.
  
####  References:
TR Golub, DK Slonim, P Tamayo, C Huard, M Gaasenbeek, JP Mesirov, H Coller, ML Loh, JR Downing, MA Caligiuri, et al. Molecular classification of cancer: class discovery and class prediction by gene expression monitoring
Science, 286 (1999), pp. 531-537

&nbsp;
&nbsp;

# II. Typical path: training + testing 

## 1. Typical workflow when training and testing data sets are comparable

When training and testing data sets are comparable, the typical workflow is:

1). select the algorithm you would like to use, there are three choices: LPS, PRPS, and PS

2). have your training data set ready, and make your decisions on the parameters

3). run LPStraining, or PRPStraining or PStraining

4). run LPStesting, or PRPStesting or PStesting


## 2. Example data

In the data folder, there are four data files.

### 1). rosenwald.cli

This data frame contains subset clinic information about rosenwald dataset, wihch is downloaded from LPS R package: https://cran.r-project.org/web/packages/LPS/LPS.pdf. The original rosenwald data set contains 240 Diffuse Large B-Cell Lymphomas samples (https://llmpp.nih.gov/DLBCL/), in this subset, however, 40 samples were randomly selected from the training set that are not in Type III sub-types, and 20 samples ere randomly selected from the validation set that are not in Type III sub-types, together there are 60 Diffuse Large B-Cell Lymphomas samples in rosenwald.cli. The column "set" indicates if a sample was in training or validation data set in the Rosenwald paper, column group is for COO (cell of origin) classification, which could be GCB (germinal center), ABC (activated B cell) , and UNC (un-classified) in the paper, however, in this subset data, we only have GCB and ABC types. 


```{r}
library(PRPS)
data("rosenwald.cli")
str(rosenwald.cli)
```

#### References:
Rosenwald A, Wright G, Chan WC, et al. The use of molecular profiling to predict survival after chemotherapy for diffuse large-B-cell lymphoma. N Engl J Med 2002;346(25):1937-1947


### 2). rosenwald.expr

This data matrix contains Lymphochip microarrays expression data for the 60 Diffuse Large B-Cell Lymphomas samples as described in rosenwald.cli. 

```{r}
data("rosenwald.expr")
str(rosenwald.expr)
```

#### References:

Rosenwald A, Wright G, Chan WC, et al. The use of molecular profiling to predict survival after chemotherapy for diffuse large-B-cell lymphoma. N Engl J Med 2002;346(25):1937-1947

### 3). WrightCOO

This data frame contains 158 COO (cell of origin) related genes with both Ensembl annotation ID (row names) and gene symbols (column "Gene"). These genes are used to classfy DLBCL samples into GCB, ABC, or UNC.

```{r}
data("WrightCOO")
str(WrightCOO)
head(WrightCOO)
```

#### References:
Wright G, Tan B, Rosenwald A, Hurt EH, Wiestner A, Staudt LM. A trait expression-based method
to diagnose clinically distinct subgroups of diffuse large B cell lymphoma. Proc Natl Acad Sci U S
A. 2003 Aug 19;100(17):9991-6.

### 4). DHITsig

```{r}
data("DHITsig")
str(DHITsig)
head(DHITsig)
```


#### References:
Ennishi D, Jiang A, Boyle M, Collinge B, Grande BM, Ben-Neriah S, Rushton C, Tang J, Thomas N, Slack GW, Farinha P, Takata K, Miyata-Takata T, Craig J, Mottok A, Meissner B, Saberi S, Bashashati A, Villa D, Savage KJ, Sehn LH, Kridel R, Mungall AJ, Marra MA, Shah SP, Steidl C, Connors JM, Gascoyne RD, Morin RD, Scott DW. Double-Hit Trait Expression Signature Defines a Distinct Subgroup of Germinal Center B-Cell-Like Diffuse Large B-Cell
Lymphoma. J Clin Oncol. 2018 Dec 3:JCO1801583. doi: 10.1200/JCO.18.01583.


## 3. Example code

In this section, we are providing example code to get classification score when training and testing data are comparable

### 1) LPS

#### LPStraining

Get the data ready:
```{r}
dat = rosenwald.expr
trainset = subset(rosenwald.cli, rosenwald.cli$set == "Training")
testset = subset(rosenwald.cli, rosenwald.cli$set == "Validation")
```
Use GCB as a reference group as in LPS package, then LPStraining result can be got as following
```{r}
nin = 100 
trainLPS = LPStraining (trainDat = dat[,rownames(trainset)], groupInfo = trainset$group, refGroup = "GCB", topN = nin,
                      weightMethod = "ttest")
str(trainLPS)

```

The last item in the output of LPStraining provides information on how our LPS prediction model is doing
```{r}
trainLPS$classTable
```

This means with given COO info, and with top 100 significant genes based on t test, LPS classification can put all GCB samples back to GCB samples, for ABC samples, however, one of them is put into UNCLASS, all other samples are put back into ABC. 

More detail information about LPStraining function can be found with the two choices:

```{r}
?(LPStraining)
help(LPStraining)
```

#### LPStesting

Now, we need to have a testing data set that is comparable to the training data set. For this simple, however, we already have testing data set shown above, and we can use the output from LPStraining together with testing data set to do LPStesting, and compare the result from the original ABC-GCB classification. 

```{r}
testLPS = LPStesting(LPStrainObj = trainLPS, newdat = dat[,rownames(testset)])
str(testLPS)
table(testLPS$LPS_class)
table(testLPS$LPS_class, testset$group)
```

We can also combine training and testing data sets together for testing

```{r}
fullLPS = LPStesting(LPStrainObj = trainLPS, newdat = dat)
str(fullLPS)
table(fullLPS$LPS_class)
```

Now, we are wondering if the testLPS and fulltest give us consistent results.

```{r}
table(testLPS$LPS_class, fullLPS[rownames(testset), "LPS_class"])

```

More detail about LPStesting can be found with:
```{r}
?(LPStesting)
help(LPStesting)
```

### 2) PRPS
#### PRPStraining

Use the same training data set as above, and use GCB as a reference group as in LPS package, then PRPStraining result can be got as following. Notice that now we are calling PRPStraining but all setting is the same as for LPStraining

```{r}
nin = 100 
trainPRPS = PRPStraining (trainDat = dat[,rownames(trainset)], groupInfo = trainset$group, refGroup = "GCB", topN = nin,
                      weightMethod = "ttest")
str(trainPRPS)

```

The last item in the output of PRPStraining provides information on how our PRPS prediction model is doing
```{r}
trainPRPS$classTable
```

This means with given COO info, and with top 100 significant genes based on t test, PRPS classification is 100% matching to given COO.

More detail information about LPStraining function can be found with the two choices:

```{r}
?(PRPStraining)
help(PRPStraining)
```

#### PRPStesting

With the exact same testing data set as above, use the output from PRPStraining, we can apply PRPStesting as following and compare the classification with the truth in the testing data info:

```{r}
testPRPS = PRPStesting(PRPStrainObj = trainPRPS, newdat = dat[,rownames(testset)])
str(testPRPS)
table(testPRPS$PRPS_class)
table(testPRPS$PRPS_class, testset$group)


```

We can also combine training and testing data sets together for testing

```{r}
fullPRPS = PRPStesting(PRPStrainObj = trainPRPS, newdat = dat)
str(fullPRPS)
table(fullPRPS$PRPS_class)
```

Now, we are wondering if the testLPS and fulltest give us consistent results.

```{r}
table(testPRPS$PRPS_class, fullPRPS[rownames(testset), "PRPS_class"])
```

If we compare PRPS results with LPS results, we can find that the training data itself is 100% matched to the truth for LPS but there is one sample put into UNCLASS based on PRPS. However, for the testing data set, there is one GCB sample was classified to ABC by LPS, while ihe same samppe is classified into UNCLASS by PRPS, which seems more reasonable. 


### 3) PS
#### PStraining

Use the same training data set as above, and use GCB as a reference group as in LPS package, then PStraining result can be got as following. Notice that now we are calling PStraining but all setting is the same as for LPStraining

```{r}
nin = 100 
trainPS = PStraining (trainDat = dat[,rownames(trainset)], groupInfo = trainset$group, refGroup = "GCB", topN = nin, weightMethod = "ttest")
str(trainPS)

```

The last item in the output of PStraining provides information on how our PS prediction model is doing
```{r}
trainPS$classTable
```

This means with given COO info, and with top 100 significant genes based on t test, PS classification is 100% matching to given COO.

More detail information about LPStraining function can be found with the two choices:
  
```{r}
?(PStraining)
help(PStraining)
```

#### PStesting

With the exact same testing data set as above, use the output from PStraining, we can apply PStesting as following:
  
```{r}
testPS = PStesting(PStrainObj = trainPS, newdat = dat[,rownames(testset)])
str(testPS)
table(testPS$PS_class)
table(testPS$PS_class, testset$group)
```

We can also combine training and testing data sets together for testing

```{r}
fullPS = PStesting(PStrainObj = trainPS, newdat = dat)
str(fullPS)
table(fullPS$PS_class)
```

Now, we are wondering if the testLPS and fulltest give us consistent results.

```{r}
table(testPS$PS_class, fullPS[rownames(testset), "PS_class"])
```

PS performs exactly the same as LPS for the above training and testing data sets. 

&nbsp;
&nbsp;


# III. Special path: classification score calculation without comparable training 

In many situations, we might not have comparable training and testing data sets, in this case, we might still calculate classification scores and make classification calls.

## 1. A similar data set with classificaiton info is available

If there is no comparable training data available, but there is a similar to testing data with classification info available, we might apply calibration or normalization or standardization to make it as our pseudo training data set. In this case, we can do the following:

i) If we believe that there is only some score shifting between training and testing data sets, calibrate classification scores between testing and traning data sets, and then use the calibrated mean and sd of two groups in the traning data set to calculate Empirical Bayes probabilities for the testing data set, and furthermore get classification calls. Notice that this approach is easy for LPS method but not easy for PROS and PS. This is because PRPS and PS require feature level group info as well, which is more difficult to deal with.

ii) If the assumption in i) does not hold, which means that there are more difference between training and testing data sets, and calibration on classification scores cannot overcome the difference, but if we have house keeping genes, we can use house keeping genes to normalize both training and testing data sets, and use the normalized training and testing data sets and follow instruction in II Typical path: training + testing. This normalization strategy can be applied for all three algorithm in this package: LPS, PRPS and PS. The assumption, however, is that these house keeping genes perform similar across training and testing data sets. When this assumption does not hold, this approach will not work.

iii) If both i) and ii) are not applicable, which means that the calibration on classification scores does not work and we cannot use house keeping genes for normalization, then, we can apply feature-wise standardization for both pseudo training and testing data sets. In this case,  when we follow instruction in II Typical path: training + testing, we just need to make sure standardization = TRUE. This approach can be worked for all three algorithms in this package: LPS, PRPS and PS. The assumption is, however, the ture distribution for each feature is similar between training and testing
data sets. If not, this approach will not work. For example, if there is a significant higher proportion of one group in the testing data set
than in the training data set, normalization method will not work. 

```{r}
lpstrain = LPStraining (trainDat = dat[,rownames(trainset)], standardization=TRUE, groupInfo = trainset$group, refGroup = "GCB", topN = nin,weightMethod = "ttest")
str(trainLPS)

lpstest = LPStesting(lpstrain, newdat = dat[,rownames(testset)], standardization = TRUE)
str(lpstest)
```

In general, this is not a big problem is there is linear shift in classification score level, or even in feature level. But we do have problem to deal with data if distribution is different between training and testing. 

## 2. No similar data set with classificaiton info is available

Not only we do not have comparable training and testing data sets, but also we do not have similar training and testing data sets. This means that we do not have any kinds of training data sets at all. In this situation, we should at least have gene features/traits that we would like to distiguish samples. Otherwise no classification score can be calculated.


### 1). Weights for selected features/traits are available

If we know selected features/traits with their weights but no any kind of training data set,  and we would like to calculate classification score for a given tesing data set, what can we do?

#### LPS 

Without any further information and without any assumption, the only method we can use to calculate classification score is to apply LPS approach. In this case, we can call getClassScores and set method as LPS.

Assuming that we have features and their weights from some where, for example, we can get this information from trainLPS:

```{r}
lpswts = trainLPS$LPS_pars$weights
str(lpswts)
```

Here, the tValue is the weight for the top 100 genes selected with LPStraining. 

```{r}
genes = rownames(lpswts)
lpswts = lpswts[,1]
names(lpswts) = genes
head(lpswts)

```

With the same *testdat*, we can get LPS score with function: getClassScores

```{r}
lpsscores = getClassScores(testdat = dat[,rownames(testset)], classMethod = "LPS", weights = lpswts)
head(lpsscores)
```

While LPS score calculation only requires weights, in order to get classification, we need more information to help us. LPS classification is based on Empirical Bayes probabilities, which requires LPS score mean and sd for two groups (for the example data, GCB or ABC). In order to get these information, we might use the following methods to make classifcation calls:

i) Assume a prior that indicate what percentage of scores can be used to calculate group mean and sd for both groups, obviously but by median is the most conservative way to do so, you migt try 1/3 for one group and 2/3 for the other group, or 1/3 in both ends.

ii) Visualize the score distribution with histogram or any other tools, and get the initial classification, then calculate score mean and sd for both groups. 

Once we have LPS scores with i) or ii), and LPS score mean and sd for the two groups, we can calcualte Empirical Bayes probabilities, and make classification calls as usual.


#### PRPS and PS

When PRPS and PS methods are chosen, we need to use initial group information to calculate PRPS and PS classification scores. Again, both prior and visualization methods can be used as for LPS appraoch. The differene is that we need this info to calculate mean and sd for the two groups for each gene level in order to get PRPS and PS scores. Again, we need to call getClassScores function:

getClassScores(testdat, classMethod = c("LPS", "PRPS", "PS"), weights,
  classMeans, classSds)


When we have these classification scores, and if we do not want to use natual cutoff of 0 to claim classification, but want to calculate Empirical Bayes probabilities, then, at the this point, we need to use initial group information again, and then make classification calls.

### 2). Weights for selected features/traits are  NOT available




#### the following section seesm not work, however, I might use approach for DLC GCB COO with self learning and extension to other cases 
and more cohorts as example, here, we do not have training and testing, and we do not have COO weights, but we do have ..., I am confused, think more later
<!-- If we only know selected features/traits, but we do not know these features' weights, and we do not have any kinds of training data sets. In this case, we might do the following -->

<!-- i) Standardize data along each feature/trait, make all feature's distribution comparable to N(0,1) -->
<!-- ii) use equal weight of 1 for all selected features/traits -->
<!-- iii) follow all steps in 1). Weights for selected features/traits are available -->


&nbsp;
&nbsp;

# IV Plots

For the training and testing objects, we can alos plot them out.

## 1. Plot training objects

For all LPS and PRPS training objects, we can plot ROC curve, histogram, and Empirical Bayesian probabilites vs classification score scatter plot. For PS training objects, only ROC curve and histogram plots are produced because PS method does not involve Empirical Bayesian probability calculation.

```{r}
plotNames = c("lpsPlots","prpsPlots","psPlots")
trainNames = paste("train",plotNames, sep="_")
plotTraining(trainObj = trainLPS, plotName = trainNames[1])
plotTraining(trainObj = trainPRPS, plotName = trainNames[2])
plotTraining(trainObj = trainPS, plotName = trainNames[3])
```

The plot files are saved with given file names. Notice that the ROC curve usually ahould be 100% AUC, this is because classification scores from one given group should be always bigger or smaller than the other group. When the scores are overlapped in order, AUC will not 100% any more.


## 2. Plot testing objects

For all LPS and PRPS training objects, we can plot histogram and Empirical Bayesian probabilites vs classification score scatter plot. For PS training objects, only histogram plots are produced because PS method does not involve Empirical Bayesian probability calculation.

```{r}
testNames = paste("test",plotNames, sep="_")
plotTesting(testObj = testLPS, plotName = testNames[1])
plotTesting(testObj = testPRPS, plotName = testNames[2])
plotTesting(testObj = testPS, plotName = testNames[3])
```

The plot files are saved with given file names.






